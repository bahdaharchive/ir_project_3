{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTR Experimentation Report\n",
    "\n",
    "This notebook creates a LTR pipeline and runs several experiments using pyterrier.\n",
    "\n",
    "\n",
    "## Pre-req\n",
    "\n",
    "- java installed\n",
    "- jupyter notebook\n",
    "- python 3\n",
    "- pip3\n",
    "- pyterrier\n",
    "- sklearn\n",
    "- numpy\n",
    "- pandas\n",
    "- matplotlib\n",
    "\n",
    "\n",
    "## How to install pyterrier\n",
    "\n",
    "```\n",
    "pip install python-terrier\n",
    "pip install --upgrade git+https://github.com/terrier-org/pyterrier.git#egg=python-terrier\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyterrier as pt\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt  # plotting libraries\n",
    "\n",
    "if not pt.started(): # initalizes pyterrir. Make sure you are using with unix\n",
    "  pt.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time() # start time of notebook -- used when running all cells to see how long it will take to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Boolean toggles whether we use the vaswani or deel learning dataset for this notebook.\n",
    "This block will download and index the deep-learning dataset if we don't already have it downloaded locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = False # vaswani / trec\n",
    "if b:\n",
    "    dataset = pt.datasets.get_dataset(\"vaswani\")\n",
    "    indexref = dataset.get_index()\n",
    "    topics = dataset.get_topics()\n",
    "    qrels = dataset.get_qrels()\n",
    "else:\n",
    "    dataset = pt.datasets.get_dataset(\"trec-deep-learning-docs\")\n",
    "    corpus = dataset.get_corpus()\n",
    "    index_path = './trec_dldocs_index'\n",
    "    if not os.path.isdir(index_path):\n",
    "        indexer = pt.TRECCollectionIndexer(index_path)\n",
    "        index_properties = {'block.indexing': 'true', 'invertedfile.lexiconscanner': 'pointers'}\n",
    "        indexer.setProperties(**index_properties)\n",
    "\n",
    "        indexref = indexer.index(dataset.get_corpus())\n",
    "    else:\n",
    "        indexref = pt.autoclass(\"org.terrier.querying.IndexRef\").of(os.path.join(index_path, \"data.properties\"))\n",
    "\n",
    "    topics = dataset.get_topics('test')\n",
    "    qrels = dataset.get_qrels('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Rankers\n",
    "\n",
    "This initializes several word frequency driven IR rankers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25 = pt.BatchRetrieve(indexref, controls = {\"wmodel\": \"BM25\"})\n",
    "TF_IDF =  pt.BatchRetrieve(indexref, controls = {\"wmodel\": \"TF_IDF\"})\n",
    "PL2 =  pt.BatchRetrieve(indexref, controls = {\"wmodel\": \"PL2\"})\n",
    "DPH = pt.BatchRetrieve(indexref, controls = {\"wmodel\": \"DPH\"})\n",
    "PL2F =  pt.BatchRetrieve(indexref, controls = {\"wmodel\": \"PL2F\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Retriever\n",
    "\n",
    "The feature retriever is reponsible for fetching documents and annotating them with features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_batch_retriever = pt.FeaturesBatchRetrieve(indexref, controls = {\"wmodel\": \"BM25\"}, features=[\"WMODEL:TF_IDF\", \"WMODEL:PL2\", \"WMODEL:DPH\", \"WMODEL:BM25\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example query with trimming operator\n",
    "(BM25 %2).transform(\"world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTR Algorithm\n",
    "\n",
    "This section of the code creates the ltr model that we re-rank our top k results with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train, test, validate split for dataset\n",
    "train_topics, valid_topics, test_topics = np.split(topics, [int(.6*len(topics)), int(.8*len(topics))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains vanilla random forest regression LTR model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "ltr_model = feature_batch_retriever >> pt.pipelines.LTR_pipeline(RandomForestRegressor(n_estimators=400))\n",
    "ltr_model.fit(train_topics, qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays table of baseline performance of LTR model compared against some of our other statistical models\n",
    "pt.pipelines.Experiment([TF_IDF, BM25, PL2, ltr_model], test_topics, qrels, [\"map\", \"ndcg\"], names=[\"TF-IDF\", \"BM25 algorithm\", \"PL2 Baseline\", \"LTR Random Forest\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentations\n",
    "\n",
    "Now that we have our LTR model, we can run experiments with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # plotting libraries\n",
    "\n",
    "# this is a simple experiment that compares two retrieval models with varying values for k\n",
    "def run_experiment(trained_model, mop, start= 10, finish = 600, incrementer = 25, top_k_model = BM25, title=\"sample title\"):\n",
    "    k_list = []\n",
    "    moe_list = []\n",
    "    for k in range(start, finish, incrementer):\n",
    "        efficient_pipeline = top_k_model % k >> trained_model\n",
    "        results = pt.pipelines.Experiment([efficient_pipeline], test_topics, qrels, [mop], names=[\"model\"])\n",
    "        k_list.append(k)\n",
    "        moe_list.append(results[mop].iloc[0])\n",
    "        \n",
    "        plt.plot(k_list, moe_list)\n",
    "        plt.xlabel(\"K\")\n",
    "        plt.ylabel(mop)\n",
    "        file_name = title.replace(\" \", \"-\")\n",
    "        plt.title(title)\n",
    "        plt.savefig(file_name)\n",
    "        \n",
    "run_experiment(ltr_model, \"ndcg\", title = \"K's Affect on NDCG in Learning to Rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(ltr_model, \"map\",incrementer=100,finish=2000, title = \"K's Affect on MAP in Learning to Rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# this experiments with varying the k value with PL2, BM25 and their LTR pipelines\n",
    "def run_experiment_execution_time(trained_model, samples = 1, start= 10, finish = 600, incrementer = 20, title=\"K Verses Time to Rank\"):\n",
    "    k_list = []\n",
    "    moe_list_mb25 = []\n",
    "    moe_list_pl2 = []\n",
    "    moe_bm25_baseline = []\n",
    "    moe_pl2_baseline = []\n",
    "    for k in range(start, finish, incrementer):\n",
    "        total = 0.0\n",
    "        for i in range(0, samples):\n",
    "            start_time = datetime.now()\n",
    "            efficient_pipeline = BM25 % k >> trained_model\n",
    "            results = pt.pipelines.Experiment([efficient_pipeline], test_topics, qrels, [\"map\"], names=[\"model\"])\n",
    "            finish_time = datetime.now()\n",
    "            elapse_time = (finish_time - start_time).total_seconds()\n",
    "            total += elapse_time\n",
    "        k_list.append(k)\n",
    "        moe_list_mb25.append(total/samples)\n",
    "        total = 0.0\n",
    "        for i in range(0, samples):\n",
    "            start_time = datetime.now()\n",
    "            efficient_pipeline = PL2 % k >> trained_model\n",
    "            results = pt.pipelines.Experiment([efficient_pipeline], test_topics, qrels, [\"map\"], names=[\"model\"])\n",
    "            finish_time = datetime.now()\n",
    "            elapse_time = (finish_time - start_time).total_seconds()\n",
    "            total += elapse_time\n",
    "        moe_list_pl2.append(total/samples)\n",
    "        \n",
    "        total = 0.0\n",
    "        for i in range(0, samples):\n",
    "            start_time = datetime.now()\n",
    "            efficient_pipeline = BM25 % k\n",
    "            results = pt.pipelines.Experiment([efficient_pipeline], test_topics, qrels, [\"map\"], names=[\"model\"])\n",
    "            finish_time = datetime.now()\n",
    "            elapse_time = (finish_time - start_time).total_seconds()\n",
    "            total += elapse_time\n",
    "        moe_bm25_baseline.append(total/samples)\n",
    "        \n",
    "        total = 0.0\n",
    "        for i in range(0, samples):\n",
    "            start_time = datetime.now()\n",
    "            efficient_pipeline = PL2 % k\n",
    "            results = pt.pipelines.Experiment([efficient_pipeline], test_topics, qrels, [\"map\"], names=[\"model\"])\n",
    "            finish_time = datetime.now()\n",
    "            elapse_time = (finish_time - start_time).total_seconds()\n",
    "            total += elapse_time\n",
    "        moe_pl2_baseline.append(total/samples)\n",
    "        \n",
    "        \n",
    "        \n",
    "    plt.plot(k_list, moe_list_mb25, label=\"LTR BM25 Pipeline\")\n",
    "    plt.plot(k_list, moe_list_pl2, label=\"LTR PL2 Pipeline\")\n",
    "    plt.plot(k_list, moe_bm25_baseline, label=\"BM25\")\n",
    "    plt.plot(k_list, moe_pl2_baseline, label=\"PL2\")\n",
    "    \n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Time Seconds\")\n",
    "    file_name = title.replace(\" \", \"-\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_execution_time(ltr_model, incrementer=20, samples=3, title=\"K Verses Time to Rank Trec Deep Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rather than displaying execution time in terms of ranking the entire corpus, this displays ranking time in terms of executing a single query.\n",
    "def run_experiment_execution_time_single_query(trained_model, samples = 1, start= 10, finish = 600, incrementer = 20, title=\"K Verses Time to Rank\"):\n",
    "    k_list = []\n",
    "    moe_list_mb25 = []\n",
    "    moe_list_pl2 = []\n",
    "    moe_bm25_baseline = []\n",
    "    moe_pl2_baseline = []\n",
    "    for k in range(start, finish, incrementer):\n",
    "        total = 0.0\n",
    "        for i in range(0, samples):\n",
    "            efficient_pipeline = BM25 % k >> trained_model\n",
    "            start_time = datetime.now()\n",
    "            results = (efficient_pipeline).transform(\"world\")\n",
    "            finish_time = datetime.now()\n",
    "            elapse_time = (finish_time - start_time).total_seconds()\n",
    "            total += elapse_time\n",
    "        k_list.append(k)\n",
    "        moe_list_mb25.append(total/samples)\n",
    "        total = 0.0\n",
    "        for i in range(0, samples):\n",
    "            efficient_pipeline = PL2 % k >> trained_model\n",
    "            start_time = datetime.now()\n",
    "            results = (efficient_pipeline).transform(\"world\")\n",
    "            finish_time = datetime.now()\n",
    "            elapse_time = (finish_time - start_time).total_seconds()\n",
    "            total += elapse_time\n",
    "        moe_list_pl2.append(total/samples)\n",
    "        \n",
    "        total = 0.0\n",
    "        for i in range(0, samples):\n",
    "            efficient_pipeline = BM25 % k\n",
    "            start_time = datetime.now()\n",
    "            results = (efficient_pipeline).transform(\"world\")\n",
    "            finish_time = datetime.now()\n",
    "            elapse_time = (finish_time - start_time).total_seconds()\n",
    "            total += elapse_time\n",
    "        moe_bm25_baseline.append(total/samples)\n",
    "        \n",
    "        total = 0.0\n",
    "        for i in range(0, samples):\n",
    "            efficient_pipeline = PL2 % k\n",
    "            start_time = datetime.now()\n",
    "            results = (efficient_pipeline).transform(\"world\")\n",
    "            finish_time = datetime.now()\n",
    "            elapse_time = (finish_time - start_time).total_seconds()\n",
    "            total += elapse_time\n",
    "        moe_pl2_baseline.append(total/samples)\n",
    "        \n",
    "        \n",
    "        \n",
    "    plt.plot(k_list, moe_list_mb25, label=\"LTR BM25 Pipeline\")\n",
    "    plt.plot(k_list, moe_list_pl2, label=\"LTR PL2 Pipeline\")\n",
    "    plt.plot(k_list, moe_bm25_baseline, label=\"BM25\")\n",
    "    plt.plot(k_list, moe_pl2_baseline, label=\"PL2\")\n",
    "    \n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Time Seconds\")\n",
    "    file_name = title.replace(\" \", \"-\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(file_name)\n",
    "run_experiment_execution_time_single_query(ltr_model, incrementer=20, samples=20, title=\"K Verses Time to Rank Trec Deep Learning Single Query Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs experiment showing performance of several models at the same time varying the values for k\n",
    "# the measure of performance (MOP) is variable\n",
    "def run_experiment_model(trained_model, mop, start= 10, finish = 600, incrementer = 25, title=\"sample title\"):\n",
    "    k_list = []\n",
    "    bm25_list = []\n",
    "    pl2_list = []\n",
    "    \n",
    "    bm25_base = []\n",
    "    pl2_base = []\n",
    "    for k in range(start, finish, incrementer):\n",
    "        efficient_pipeline_bm_52 = BM25 % k >> trained_model\n",
    "        results = pt.pipelines.Experiment([efficient_pipeline_bm_52], test_topics, qrels, [mop], names=[\"model\"])\n",
    "        k_list.append(k)\n",
    "        bm25_list.append(results[mop].iloc[0])\n",
    "        \n",
    "        efficient_pipeline_pl2 = PL2 % k >> trained_model\n",
    "        results = pt.pipelines.Experiment([efficient_pipeline_pl2], test_topics, qrels, [mop], names=[\"model\"])\n",
    "        pl2_list.append(results[mop].iloc[0])\n",
    "        \n",
    "        \n",
    "        efficient_pipeline_bm25 = BM25 % k\n",
    "        results = pt.pipelines.Experiment([efficient_pipeline_bm25], test_topics, qrels, [mop], names=[\"model\"])\n",
    "        bm25_base.append(results[mop].iloc[0])\n",
    "        \n",
    "        efficient_pipeline_pl2 = PL2 % k\n",
    "        results = pt.pipelines.Experiment([efficient_pipeline_pl2], test_topics, qrels, [mop], names=[\"model\"])\n",
    "        pl2_base.append(results[mop].iloc[0])\n",
    "        \n",
    "    plt.plot(k_list, bm25_list, label=\"LTR BM25 Pipeline\")\n",
    "    plt.plot(k_list, pl2_list, label=\"LTR PL2 Pipeline\")\n",
    "    plt.plot(k_list, bm25_base, label=\"BM25\")\n",
    "    plt.plot(k_list, pl2_base, label=\"PL2\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(mop)\n",
    "    file_name = title.replace(\" \", \"-\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_model(ltr_model,\"ndcg\", title=\"NDCG vs K Trec Deep Learning\")       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment_model(ltr_model,\"map\", title=\"MAP vs K Trec Deep Learning\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays total time it takes for this notebook to run\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
